{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-threading with ThreadsX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia",
      "language": "julia",
      "name": "julia"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macorony/Workshop-of-Parallel-Programming-in-Julia/blob/main/Multi_threading_with_ThreadsX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ1r1bbb0yBv"
      },
      "source": [
        "# <img src=\"https://github.com/JuliaLang/julia-logo-graphics/raw/master/images/julia-logo-color.png\" height=\"100\" /> _Colab Notebook Template_\n",
        "\n",
        "## Instructions\n",
        "1. Work on a copy of this notebook: _File_ > _Save a copy in Drive_ (you will need a Google account). Alternatively, you can download the notebook using _File_ > _Download .ipynb_, then upload it to [Colab](https://colab.research.google.com/).\n",
        "2. If you need a GPU: _Runtime_ > _Change runtime type_ > _Harware accelerator_ = _GPU_.\n",
        "3. Execute the following cell (click on it and press Ctrl+Enter) to install Julia, IJulia and other packages (if needed, update `JULIA_VERSION` and the other parameters). This takes a couple of minutes.\n",
        "4. Reload this page (press Ctrl+R, or ⌘+R, or the F5 key) and continue to the next section.\n",
        "\n",
        "_Notes_:\n",
        "* If your Colab Runtime gets reset (e.g., due to inactivity), repeat steps 2, 3 and 4.\n",
        "* After installation, if you want to change the Julia version or activate/deactivate the GPU, you will need to reset the Runtime: _Runtime_ > _Factory reset runtime_ and repeat steps 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIeFXS0F0zww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8cd19d1e-3ee3-4678-827c-0b683ff8715c"
      },
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.7.1\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools Plots\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -n \"$COLAB_GPU\" ] && [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  if [ \"$COLAB_GPU\" = \"1\" ]; then\n",
        "      JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/latex": "Unrecognized magic \\texttt{\\%\\%shell}.\n\nJulia does not use the IPython \\texttt{\\%magic} syntax.   To interact with the IJulia kernel, use \\texttt{IJulia.somefunction(...)}, for example.  Julia macros, string macros, and functions can be used to accomplish most of the other functionalities of IPython magics.\n\n",
            "text/markdown": "Unrecognized magic `%%shell`.\n\nJulia does not use the IPython `%magic` syntax.   To interact with the IJulia kernel, use `IJulia.somefunction(...)`, for example.  Julia macros, string macros, and functions can be used to accomplish most of the other functionalities of IPython magics.\n",
            "text/plain": [
              "  Unrecognized magic \u001b[36m%%shell\u001b[39m.\n",
              "\n",
              "  Julia does not use the IPython \u001b[36m%magic\u001b[39m syntax. To interact with the IJulia\n",
              "  kernel, use \u001b[36mIJulia.somefunction(...)\u001b[39m, for example. Julia macros, string\n",
              "  macros, and functions can be used to accomplish most of the other\n",
              "  functionalities of IPython magics."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OS3Ac017T1i"
      },
      "source": [
        "# Checking the Installation\n",
        "The `versioninfo()` function should print your Julia version and some other info about the system:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEzvvzCl1i0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c6efdc-bcbb-4665-e4f6-5a8192d17265"
      },
      "source": [
        "versioninfo()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Julia Version 1.7.1\n",
            "Commit ac5cc99908 (2021-12-22 19:35 UTC)\n",
            "Platform Info:\n",
            "  OS: Linux (x86_64-pc-linux-gnu)\n",
            "  CPU: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "  WORD_SIZE: 64\n",
            "  LIBM: libopenlibm\n",
            "  LLVM: libLLVM-12.0.1 (ORCJIT, broadwell)\n",
            "Environment:\n",
            "  JULIA_NUM_THREADS = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n",
        "\n",
        "M = rand(2^11, 2^11)\n",
        "\n",
        "@btime $M * $M;"
      ],
      "metadata": {
        "id": "YjM_qq54lCcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ad87e1-99ba-4199-e2cc-088b0945b686"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  471.490 ms (2 allocations: 32.00 MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XciCcMAJOT3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16113129-fd12-4467-9f31-8c80d9a73996"
      },
      "source": [
        "if ENV[\"COLAB_GPU\"] == \"1\"\n",
        "    using CUDA\n",
        "\n",
        "    run(`nvidia-smi`)\n",
        "\n",
        "    # Create a new random matrix directly on the GPU:\n",
        "    M_on_gpu = CUDA.CURAND.rand(2^11, 2^11)\n",
        "    @btime $M_on_gpu * $M_on_gpu; nothing\n",
        "else\n",
        "    println(\"No GPU found.\")\n",
        "end"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RC1QNNqk6h1"
      },
      "source": [
        "# Need Help?\n",
        "\n",
        "* Learning: https://julialang.org/learning/\n",
        "* Documentation: https://docs.julialang.org/\n",
        "* Questions & Discussions:\n",
        "  * https://discourse.julialang.org/\n",
        "  * http://julialang.slack.com/\n",
        "  * https://stackoverflow.com/questions/tagged/julia\n",
        "\n",
        "If you ever ask for help or file an issue about Julia, you should generally provide the output of `versioninfo()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMidUQB03vJ"
      },
      "source": [
        "Add new code cells by clicking the `+ Code` button (or _Insert_ > _Code cell_).\n",
        "\n",
        "Have fun!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-mask.png\" height=\"100\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallelizing with ThreadsX.mapreduce\n",
        "ThreadsX is a multi-threaded Julia library that provides parallel versions of some of the Base functions. "
      ],
      "metadata": {
        "id": "UdFHqWpl7Ulr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "function digitsin(digits::Int, num) \n",
        "    base = 10\n",
        "    while (digits ÷ base > 0) \n",
        "        base *= 10\n",
        "    end\n",
        "    while num > 0\n",
        "        if (num % base) == digits     \n",
        "            return true\n",
        "        end\n",
        "        num ÷= 10\n",
        "    end\n",
        "    return false\n",
        "end"
      ],
      "metadata": {
        "id": "P1_c4q6-7piQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0702ea87-d32a-469f-8336-ee714a1fca5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "digitsin (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import Pkg; Pkg.add(\"ThreadsX\")"
      ],
      "metadata": {
        "id": "HPPjAUii9zKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n",
        "using ThreadsX"
      ],
      "metadata": {
        "id": "W4mQB4T39N8Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function slow(n::Int64, digits::Int)\n",
        "  total = ThreadsX.mapreduce(+, 1:n) do i\n",
        "    if !digitsin(digits, i)\n",
        "      1.0 / i\n",
        "    else\n",
        "      0.0\n",
        "    end\n",
        "  end\n",
        "  return total\n",
        "end\n",
        "total = @btime slow(Int64(1e9), 9)\n",
        "println(\"total = \", total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1he0I-KG9hFB",
        "outputId": "b7993f51-09c7-4aa1-e450-bdacd038b8d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  24.646 s (146 allocations: 8.75 KiB)\n",
            "total = 14.241913010383238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n",
        "@btime sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:1_000_000_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88h26xyq-w2_",
        "outputId": "6596d0a9-a6d7-4692-f811-72dd313975fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  30.016 s (0 allocations: 0 bytes)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.2419130103833"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using BenchmarkTools, ThreadsX\n",
        "@btime ThreadsX.sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:1_000_000_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dMDt3IkE0u8",
        "outputId": "d593f336-7ed4-4b10-fef4-3b1e89901ccc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  23.634 s (130 allocations: 8.27 KiB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.241913010383238"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function numericTerm(i)\n",
        "  !digitsin(9, i) ? 1.0/i : 0\n",
        "end\n",
        "@btime ThreadsX.sum(numericTerm, 1:Int64(1e9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X3VAcuVFD1B",
        "outputId": "1ff1b804-45ad-4ec8-a97e-91b945cab870"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  23.930 s (130 allocations: 8.27 KiB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.241913010383238"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = Int64(1e8)\n",
        "r = rand(Float32, (n))\n",
        "r[1:10]\n",
        "last(r,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpnOGZFOGTBZ",
        "outputId": "dd48c895-e337-420c-a0a5-dcdae7b02770"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10-element Vector{Float32}:\n",
              " 0.8472833\n",
              " 0.18850714\n",
              " 0.45455682\n",
              " 0.9774078\n",
              " 0.17127883\n",
              " 0.80731434\n",
              " 0.35670525\n",
              " 0.77119374\n",
              " 0.110624015\n",
              " 0.17942017"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@btime sort(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGTqA4x4HRkF",
        "outputId": "256f8da5-75c3-44d7-cc6b-e101571492a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  13.187 s (2 allocations: 381.47 MiB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000000-element Vector{Float32}:\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 1.1920929f-7\n",
              " ⋮\n",
              " 0.9999999\n",
              " 0.9999999\n",
              " 0.9999999\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@btime ThreadsX.sort(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKKhaCTRI0uP",
        "outputId": "0edaea33-5d6b-488e-dd82-885e1da1d512"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  9.572 s (2686866 allocations: 1.07 GiB)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000000-element Vector{Float32}:\n",
              " 0.0\n",
              " 0.0\n",
              " 0.0\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 5.9604645f-8\n",
              " 1.1920929f-7\n",
              " ⋮\n",
              " 0.9999999\n",
              " 0.9999999\n",
              " 0.9999999\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994\n",
              " 0.99999994"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OOsqKu3pI6s7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}